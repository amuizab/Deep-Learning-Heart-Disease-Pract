# -*- coding: utf-8 -*-
"""Copy of Deep Learning Heart Disease Pract.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UG7Vva5S9dTdjH0zBsz5987XLy3JTgbm
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense
from tensorflow.keras import layers, optimizers
#from sklearn import s.vm, datasets

names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'Class'] 
data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data', names=names)
#data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.switzerland.data', names=names)
data

data.x11.value_counts()

data.x12.value_counts()

data.x13.value_counts()

outliers = data.x13[(data.x13 == '?')]
print(outliers)

data.x13 = data.x13.replace(to_replace =['?'], value =np.nan)
data.x12 = data.x12.replace(to_replace =['?'], value =np.nan)

data.x13.isnull().any()

data.Class.value_counts()

data.Class = data.Class.replace(to_replace =[1, 2, 3, 4], value =1)

data = data.dropna()
data

data.Class.value_counts()

sns.displot(data.Class, kind='hist')

data.dtypes

data.x12 = data.x12.astype(float)
data.x13 = data.x13.astype(float)

data.dtypes

from sklearn.preprocessing import StandardScaler
def standardscal(s,c,cc):
  x = (cc+'sc')
  sc = StandardScaler()
  c[x] = sc.fit_transform(s)
  c.pop(cc)
  return c[x]

standardscal((data[['x1']]), data, ('x1'))
standardscal((data[['x2']]), data, ('x2'))
standardscal((data[['x3']]), data, ('x3'))
standardscal((data[['x4']]), data, ('x4'))
standardscal((data[['x5']]), data, ('x5'))
standardscal((data[['x6']]), data, ('x6'))
standardscal((data[['x7']]), data, ('x7'))
standardscal((data[['x8']]), data, ('x8'))

standardscal((data[['x9']]), data, ('x9'))
standardscal((data[['x10']]), data, ('x10'))
standardscal((data[['x11']]), data, ('x11'))
standardscal((data[['x12']]), data, ('x12'))
standardscal((data[['x13']]), data, ('x13'))

data.dtypes

data.corr()

#data = data.drop(['x2','x5','x6','x9'],axis=1)
data = data.drop(['x5sc','x6sc','x8sc'],axis=1)

data.dtypes

data.isnull().any()

data.corr()

"""# Model Deep Learning"""

fitur = data.drop(columns='Class').values
label = data['Class']

X_train, X_test, y_train, y_test = train_test_split(fitur, label, test_size=0.3, random_state=20)

"""## 2 Layers, Relu Sigmoid, lr=0.01, Epoch 200, Batch 32"""

model = Sequential()
model.add(Dense(50, input_dim=10, activation='relu')) 
'''model.add(Dense(15, activation='relu')) 
model.add(Dense(10, activation='relu')) 
model.add(Dense(5, activation='relu'))'''
model.add(Dense(1, activation='sigmoid'))

model.compile(loss="binary_crossentropy", optimizer=optimizers.SGD(learning_rate=0.01), metrics=['accuracy','mae', 'mse'])

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, batch_size=32, verbose=1)

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='Valid accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='Valid')
plt.legend()
plt.show()

plt.plot(history.history['mse'], label='mse')
plt.plot(history.history['val_mse'], label='Valid mse')
plt.legend()
plt.show()

"""## 2 Layers, Relu Sigmoid, lr=0.001, Epochs 400, Batch 8"""

model = Sequential()
model.add(Dense(50, input_dim=10, activation='relu')) 
model.add(Dense(1, activation='sigmoid'))

model.compile(loss="binary_crossentropy", optimizer=optimizers.SGD(learning_rate=0.001), metrics=['accuracy','mae', 'mse'])

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=400, batch_size=8, verbose=1)

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='Valid accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='Valid')
plt.legend()
plt.show()

plt.plot(history.history['mse'], label='mse')
plt.plot(history.history['val_mse'], label='Valid mse')
plt.legend()
plt.show()

"""## 2 Layers, Sigmoid Sigmoid, lr=0.1, Epochs 400, Batch 32"""

model = Sequential()
model.add(Dense(50, input_dim=10, activation='sigmoid')) 
'''model.add(Dense(15, activation='relu')) 
model.add(Dense(10, activation='relu')) 
model.add(Dense(5, activation='relu'))'''
model.add(Dense(1, activation='sigmoid'))

model.compile(loss="binary_crossentropy", optimizer=optimizers.SGD(learning_rate=0.1), metrics=['accuracy','mae', 'mse'])

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=400, batch_size=32, verbose=1)

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='Valid accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='Valid')
plt.legend()
plt.show()

plt.plot(history.history['mse'], label='mse')
plt.plot(history.history['val_mse'], label='Valid mse')
plt.legend()
plt.show()

"""## 2 Layers, Relu Sigmoid, lr=0.001, Epochs 400, Batch 16"""

model = Sequential()
model.add(Dense(50, input_dim=10, activation='relu')) 
model.add(Dense(1, activation='sigmoid'))

model.compile(loss="binary_crossentropy", optimizer=optimizers.SGD(learning_rate=0.001), metrics=['accuracy','mae', 'mse'])

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=400, batch_size=16, verbose=1)

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='Valid accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='Valid')
plt.legend()
plt.show()

plt.plot(history.history['mse'], label='mse')
plt.plot(history.history['val_mse'], label='Valid mse')
plt.legend()
plt.show()

"""## 4 Layers, Relu Relu Relu Sigmoid, lr=0.001, Epochs 400, Batch 16"""

model = Sequential()
model.add(Dense(50, input_dim=10, activation='relu')) 
model.add(Dense(25, activation='relu')) 
model.add(Dense(5, activation='relu')) 
model.add(Dense(1, activation='sigmoid'))

model.compile(loss="binary_crossentropy", optimizer=optimizers.SGD(learning_rate=0.001), metrics=['accuracy','mae', 'mse'])

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=400, batch_size=16, verbose=1)

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='Valid accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='Valid')
plt.legend()
plt.show()

plt.plot(history.history['mse'], label='mse')
plt.plot(history.history['val_mse'], label='Valid mse')
plt.legend()
plt.show()

"""## 3 Layers, Relu Relu Sigmoid, lr=0.01, Epochs 200, Batch 32, mae"""

model = Sequential()
model.add(Dense(15, input_dim=10, activation='relu')) 
model.add(Dense(10, activation='relu')) 
model.add(Dense(1, activation='linear'))

model.compile(loss="mean_absolute_error", optimizer=optimizers.SGD(learning_rate=0.01), metrics=['accuracy','mae', 'mse'])

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, batch_size=32, verbose=1)

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='Valid accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='Valid')
plt.legend()
plt.show()

plt.plot(history.history['mse'], label='mse')
plt.plot(history.history['val_mse'], label='Valid mse')
plt.legend()
plt.show()

"""## 1 Layer, Sigmoid, adam, Epochs 200, Batch 32

"""

model = Sequential()
model.add(Dense(1, activation='sigmoid'))

model.compile(loss="binary_crossentropy", optimizer='adam', metrics=['accuracy','mae', 'mse'])

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, batch_size=32, verbose=1)

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='Valid accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='Valid')
plt.legend()
plt.show()

plt.plot(history.history['mse'], label='mse')
plt.plot(history.history['val_mse'], label='Valid mse')
plt.legend()
plt.show()

"""## 1 Layer, Sigmoid, adam, Epochs 400, Batch 32

"""

model = Sequential()
model.add(Dense(1, activation='sigmoid'))

model.compile(loss="binary_crossentropy", optimizer='adam', metrics=['accuracy','mae', 'mse'])

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=400, batch_size=32, verbose=1)

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='Valid accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='Valid')
plt.legend()
plt.show()

plt.plot(history.history['mse'], label='mse')
plt.plot(history.history['val_mse'], label='Valid mse')
plt.legend()
plt.show()

"""## 1 Layer, Sigmoid, lr=0.01, Epochs 200, Batch 32

"""

model = Sequential()
model.add(Dense(1, activation='sigmoid'))

model.compile(loss="binary_crossentropy", optimizer=optimizers.SGD(learning_rate=0.01), metrics=['accuracy','mae', 'mse'])

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, batch_size=32, verbose=1)

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='Valid accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='Valid')
plt.legend()
plt.show()

plt.plot(history.history['mse'], label='mse')
plt.plot(history.history['val_mse'], label='Valid mse')
plt.legend()
plt.show()

"""## 1 Layer, Sigmoid, lr=0.01, Epochs 400, Batch 32

"""

model = Sequential()
model.add(Dense(1, activation='sigmoid'))

model.compile(loss="binary_crossentropy", optimizer=optimizers.SGD(learning_rate=0.01), metrics=['accuracy','mae', 'mse'])

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=400, batch_size=32, verbose=1)

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='Valid accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='Valid')
plt.legend()
plt.show()

plt.plot(history.history['mse'], label='mse')
plt.plot(history.history['val_mse'], label='Valid mse')
plt.legend()
plt.show()

"""## 1 Layer, Sigmoid, lr=0.01, Epochs 200, Batch 64

"""

model = Sequential()
model.add(Dense(1, activation='sigmoid'))

model.compile(loss="binary_crossentropy", optimizer=optimizers.SGD(learning_rate=0.01), metrics=['accuracy','mae', 'mse'])

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, batch_size=64, verbose=1)

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='Valid accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='Valid')
plt.legend()
plt.show()

plt.plot(history.history['mse'], label='mse')
plt.plot(history.history['val_mse'], label='Valid mse')
plt.legend()
plt.show()